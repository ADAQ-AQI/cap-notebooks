{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Uploading MOASA Flight Data\n",
    "This notebook illustrates the steps involved in uploading the Met Office Atmospheric Survey Aircraft flight data to the Clean Air data store.\n",
    "\n",
    "\n",
    "It is written to run on an internal Met Office system and access data in a particular folder location with a specific structure. As a result, the code to locate the data is not generalised and not expected to be reusable. However, the steps where the metadata is extracted and data uploaded is likely to be helpful to those who are looking to do similar with their own data.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Config"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "AIRCRAFT_DATA_LOCATION = '/project/obr/CleanAir'\n",
    "OBJECT_STORE_BUCKET = 'caf-data'  # Use this for uploading to the live data storage location\n",
    "# OBJECT_STORE_BUCKET = 'caf-test'  # Use this for testing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imports & Helper Functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Dict, Generator, Tuple\n",
    "\n",
    "\n",
    "def find_aircraft_data() -> Generator[Tuple[str, Path], None, None]:\n",
    "    \"\"\"\n",
    "    Yields the directories containing netcdf files we're interested in.\n",
    "    Uses a generator, so we can process results more efficiently, not having to traverse all the folders before getting any results.\n",
    "\n",
    "    Assumes data is organised in this directory structure: /project/obr/CleanAir/{dataset_name}/processed/\n",
    "\n",
    "    Specifically is looking for netcdf files in a directory called 'processed' and filters out files with 'old' or 'OLD' in the path\n",
    "    or files within directories named 'raw'.\n",
    "    \"\"\"\n",
    "    data_file_path = Path(AIRCRAFT_DATA_LOCATION)\n",
    "    for ds_path in data_file_path.glob('**/processed/*.nc'):\n",
    "        path_str = str(ds_path)\n",
    "        if any(bad_str in path_str for bad_str in {'old', 'OLD', '/raw/', '_pbp_'}):\n",
    "            continue\n",
    "\n",
    "        yield ds_path\n",
    "\n",
    "\n",
    "def get_dataset_name(ds_path: Path) -> str:\n",
    "    \"\"\"\n",
    "    Gets the name to use for the dataset, based on the path.\n",
    "    Assumes data is organised in this directory structure: /project/obr/CleanAir/{dataset_name}/processed/\n",
    "    \"\"\"\n",
    "    return ds_path.parts[4] if ds_path.is_absolute() else str(ds_path.parent.stem)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Find & Load the Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import warnings\n",
    "import iris, iris.cube\n",
    "\n",
    "cubes: Dict[str, iris.cube.Cube] = {}\n",
    "paths: Dict[str, Path] = {}\n",
    "loading_errors: Dict[str, Exception] = {}\n",
    "\n",
    "for path in find_aircraft_data():\n",
    "\n",
    "    ds_name = get_dataset_name(path)\n",
    "    try:\n",
    "        # Temporarily hide iris warnings from the output, as they get in the way\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\", UserWarning)\n",
    "            cubes[ds_name] = iris.load(str(path))\n",
    "            paths[ds_name] = path  # Only store this if iris loaded the data successfully\n",
    "\n",
    "    except Exception as e:\n",
    "        loading_errors[path] = e\n",
    "        print(f\"Failed to load {path} due to {repr(e)}\")\n",
    "print(f\"Loaded {len(cubes)}/{len(cubes) + len(loading_errors)} datasets\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Extract Metadata"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from clean_air.models import Metadata\n",
    "from clean_air.data.extract_metadata import extract_metadata\n",
    "\n",
    "metadata_dict: Dict[str, Metadata] = {}\n",
    "extraction_errors: Dict[str, Exception] = {}\n",
    "for ds_name, cube in cubes.items():\n",
    "    try:\n",
    "        metadata_dict[ds_name] = extract_metadata(cube, ds_name, ['clean_air:type=aircraft'], [], [])\n",
    "    except Exception as e:\n",
    "        extraction_errors[ds_name] = e\n",
    "        print(f\"Failed to extract metadata for {ds_name} due to {repr(e)}\")\n",
    "\n",
    "print(f\"Converted {len(metadata_dict)}/{len(cubes)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create DataSets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from clean_air.models import DataSet\n",
    "\n",
    "datasets: Dict[str, DataSet] = {}\n",
    "\n",
    "for ds_name, metadata in metadata_dict.items():\n",
    "    ds_file = paths[ds_name]\n",
    "    ds = DataSet([ds_file], metadata)\n",
    "    datasets[ds_name] = ds\n",
    "    print(f\"Created {ds}\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Upload DataSets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from clean_air.data.storage import create_dataset_store\n",
    "\n",
    "dataset_store = create_dataset_store(OBJECT_STORE_BUCKET, anon=False)\n",
    "\n",
    "for ds_name, ds in datasets.items():\n",
    "    print(f\"Uploading {ds}...\", end=\"\")\n",
    "    dataset_store.put(ds)\n",
    "    print(\"... Successful\")\n",
    "\n",
    "print(\"\")\n",
    "print(f\"Uploaded {len(datasets)} datasets to {OBJECT_STORE_BUCKET}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
