{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uploading AQUM Hourly O3 Data\n",
    "\n",
    "Similar to the 'uploading_moasa_flight_data' notebook, this notebook explains the steps taken to upload some model data to the Clean Air Data store.\n",
    "\n",
    "However, in this case we are first the raw netCDF's from the object store, and then reuploading them with their associated metadata profiles to the live data storage in a sibling location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs\n",
    "import iris\n",
    "import os\n",
    "from typing import Dict\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discover the Data\n",
    "`anon=False` means you will have to set up your access credentials before being able to get the data - see the notebook 'access_datastore' for help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_kwargs = {\"endpoint_url\": \"https://caf-o.s3-ext.jc.rl.ac.uk\"}\n",
    "fs = s3fs.S3FileSystem(anon=False, client_kwargs=client_kwargs)\n",
    "files = fs.ls('aqum-hourly-o3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the Data\n",
    "These files are pretty big, so we're only downloading those that aren't already in our local `temp_holder/` directory, or our `caf-data` store. \n",
    "\n",
    "Use `number_of_files` to specify how many files you want to download at a time, rather than doing them all at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_files = 2\n",
    "\n",
    "caf_data = fs.ls('caf-data')\n",
    "uploaded_data = []\n",
    "for file in caf_data:\n",
    "\tuploaded_data.append(Path(file).name)\n",
    "\n",
    "i = 0\n",
    "for file in files:\n",
    "\tif (Path(file).name not in os.listdir('./temp_holder/') and \n",
    "\t\tPath(file).stem not in uploaded_data and \n",
    "\t\ti < number_of_files):\n",
    "\t\ti += 1\n",
    "\t\tfs.get(file, './temp_holder/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Data\n",
    "From here on the notebook is mostly identical to 'uploading_moasa_flight_data'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded path temp_holder/aqum_hourly_o3_20190401.nc\n",
      "loaded path temp_holder/aqum_hourly_o3_20190326.nc\n",
      "loaded path temp_holder/aqum_hourly_o3_20190327.nc\n",
      "loaded path temp_holder/aqum_hourly_o3_20190328.nc\n",
      "loaded path temp_holder/aqum_hourly_o3_20190330.nc\n",
      "loaded path temp_holder/aqum_hourly_o3_20190329.nc\n",
      "loaded path temp_holder/aqum_hourly_o3_20190331.nc\n",
      "Loaded 7/7 datasets\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import iris, iris.cube\n",
    "\n",
    "cubes: Dict[str, iris.cube.Cube] = {}\n",
    "paths: Dict[str, Path] = {}\n",
    "loading_errors: Dict[str, Exception] = {}\n",
    "\n",
    "for path in Path('./temp_holder').glob('*.nc'):\n",
    "    ds_name = Path(path).stem\n",
    "    try:\n",
    "        # Temporarily hide iris warnings from the output, as they get in the way\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\", UserWarning)\n",
    "            cubes[ds_name] = iris.load(str(path))\n",
    "            paths[ds_name] = path  # Only store this if iris loaded the data successfully\n",
    "            print(f'loaded path {path}')\n",
    "\n",
    "    except Exception as e:\n",
    "        loading_errors[path] = e\n",
    "        print(f\"Failed to load {path} due to {repr(e)}\")\n",
    "print(f\"Loaded {len(cubes)}/{len(cubes) + len(loading_errors)} datasets\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 7/7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/h04/twilson/.conda/envs/cap_env/lib/python3.8/site-packages/iris/coord_systems.py:525: UserWarning: The default value for the *approx* keyword argument to TransverseMercator will change from True to False after 0.18.\n",
      "  return ccrs.TransverseMercator(\n"
     ]
    }
   ],
   "source": [
    "from clean_air.models import Metadata\n",
    "from clean_air.data.extract_metadata import extract_metadata\n",
    "\n",
    "metadata_dict: Dict[str, Metadata] = {}\n",
    "extraction_errors: Dict[str, Exception] = {}\n",
    "for ds_name, cube in cubes.items():\n",
    "    try:\n",
    "        metadata_dict[ds_name] = extract_metadata(cube, ds_name, \n",
    "            ['clean_air:type=gridded', \n",
    "            'clean_air:horizontal_coverage=UK', \n",
    "            'clean_air:horizontal_resolution=0.11 degree', \n",
    "            'clean_air:vertical_resolution=63 Levels'], [], ['PP'], 'AQUM Forecast')\n",
    "    except Exception as e:\n",
    "        extraction_errors[ds_name] = e\n",
    "        print(f\"Failed to extract metadata for {ds_name} due to {repr(e)}\")\n",
    "\n",
    "print(f\"Converted {len(metadata_dict)}/{len(cubes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the Metadata\n",
    "Use this to probe any metadata details you're interested in, before uploading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ScalarBounds(lower=real_datetime(2019, 3, 26, 0, 59, 59, 999987), upper=real_datetime(2019, 3, 27, 0, 0))\n"
     ]
    }
   ],
   "source": [
    "name = 'aqum_hourly_o3_20190326'\n",
    "\n",
    "if name in metadata_dict.keys():\n",
    "\tprint(metadata_dict[name].extent.temporal.bounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DataSets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created aqum_hourly_o3_20190401\n",
      "Created aqum_hourly_o3_20190326\n",
      "Created aqum_hourly_o3_20190327\n",
      "Created aqum_hourly_o3_20190328\n",
      "Created aqum_hourly_o3_20190330\n",
      "Created aqum_hourly_o3_20190329\n",
      "Created aqum_hourly_o3_20190331\n"
     ]
    }
   ],
   "source": [
    "from clean_air.models import DataSet\n",
    "\n",
    "datasets: Dict[str, DataSet] = {}\n",
    "\n",
    "for ds_name, metadata in metadata_dict.items():\n",
    "    ds_file = paths[ds_name]\n",
    "    ds = DataSet([ds_file], metadata)\n",
    "    datasets[ds_name] = ds\n",
    "    print(f\"Created {ds.metadata.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload DataSets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading aqum_hourly_o3_20190401...... Successful\n",
      "Uploading aqum_hourly_o3_20190326...... Successful\n",
      "Uploading aqum_hourly_o3_20190327...... Successful\n",
      "Uploading aqum_hourly_o3_20190328...... Successful\n",
      "Uploading aqum_hourly_o3_20190330...... Successful\n",
      "Uploading aqum_hourly_o3_20190329...... Successful\n",
      "Uploading aqum_hourly_o3_20190331...... Successful\n",
      "\n",
      "Uploaded 7 datasets to caf-data\n"
     ]
    }
   ],
   "source": [
    "from clean_air.data.storage import create_dataset_store\n",
    "\n",
    "dataset_store = create_dataset_store('caf-data', anon=False)\n",
    "\n",
    "for ds_name, ds in datasets.items():\n",
    "    try:\n",
    "        print(f\"Uploading {ds.metadata.id}...\", end=\"\")\n",
    "        dataset_store.put(ds)\n",
    "        print(\"... Successful\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to upload {ds.metadata.id} due to {repr(e)}\")\n",
    "\n",
    "print(\"\")\n",
    "print(f\"Uploaded {len(datasets)} datasets to caf-data\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('cap_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "87b966479b9aa4c0ee8b47762b9da59e19a9e60a102d788ba55adae756e42394"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
